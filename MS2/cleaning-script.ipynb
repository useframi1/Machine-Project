{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2 - Cleaning, Processing, and Data Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include all the necessary imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from datetime import timedelta\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "from fitter import Fitter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from distfit import distfit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the csv into a pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Flight_delay.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the df info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the head of the df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>Airline</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>TailNum</th>\n",
       "      <th>ActualElapsedTime</th>\n",
       "      <th>...</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>CancellationCode</th>\n",
       "      <th>Diverted</th>\n",
       "      <th>CarrierDelay</th>\n",
       "      <th>WeatherDelay</th>\n",
       "      <th>NASDelay</th>\n",
       "      <th>SecurityDelay</th>\n",
       "      <th>LateAircraftDelay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>03-01-2019</td>\n",
       "      <td>1829</td>\n",
       "      <td>1959</td>\n",
       "      <td>1925</td>\n",
       "      <td>WN</td>\n",
       "      <td>Southwest Airlines Co.</td>\n",
       "      <td>3920</td>\n",
       "      <td>N464WN</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>03-01-2019</td>\n",
       "      <td>1937</td>\n",
       "      <td>2037</td>\n",
       "      <td>1940</td>\n",
       "      <td>WN</td>\n",
       "      <td>Southwest Airlines Co.</td>\n",
       "      <td>509</td>\n",
       "      <td>N763SW</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>03-01-2019</td>\n",
       "      <td>1644</td>\n",
       "      <td>1845</td>\n",
       "      <td>1725</td>\n",
       "      <td>WN</td>\n",
       "      <td>Southwest Airlines Co.</td>\n",
       "      <td>1333</td>\n",
       "      <td>N334SW</td>\n",
       "      <td>121</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>03-01-2019</td>\n",
       "      <td>1452</td>\n",
       "      <td>1640</td>\n",
       "      <td>1625</td>\n",
       "      <td>WN</td>\n",
       "      <td>Southwest Airlines Co.</td>\n",
       "      <td>675</td>\n",
       "      <td>N286WN</td>\n",
       "      <td>228</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>03-01-2019</td>\n",
       "      <td>1323</td>\n",
       "      <td>1526</td>\n",
       "      <td>1510</td>\n",
       "      <td>WN</td>\n",
       "      <td>Southwest Airlines Co.</td>\n",
       "      <td>4</td>\n",
       "      <td>N674AA</td>\n",
       "      <td>123</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DayOfWeek        Date  DepTime  ArrTime  CRSArrTime UniqueCarrier  \\\n",
       "0          4  03-01-2019     1829     1959        1925            WN   \n",
       "1          4  03-01-2019     1937     2037        1940            WN   \n",
       "2          4  03-01-2019     1644     1845        1725            WN   \n",
       "3          4  03-01-2019     1452     1640        1625            WN   \n",
       "4          4  03-01-2019     1323     1526        1510            WN   \n",
       "\n",
       "                  Airline  FlightNum TailNum  ActualElapsedTime  ...  TaxiIn  \\\n",
       "0  Southwest Airlines Co.       3920  N464WN                 90  ...       3   \n",
       "1  Southwest Airlines Co.        509  N763SW                240  ...       3   \n",
       "2  Southwest Airlines Co.       1333  N334SW                121  ...       6   \n",
       "3  Southwest Airlines Co.        675  N286WN                228  ...       7   \n",
       "4  Southwest Airlines Co.          4  N674AA                123  ...       4   \n",
       "\n",
       "   TaxiOut  Cancelled  CancellationCode Diverted CarrierDelay WeatherDelay  \\\n",
       "0       10          0                 N        0            2            0   \n",
       "1        7          0                 N        0           10            0   \n",
       "2        8          0                 N        0            8            0   \n",
       "3        8          0                 N        0            3            0   \n",
       "4        9          0                 N        0            0            0   \n",
       "\n",
       "  NASDelay  SecurityDelay  LateAircraftDelay  \n",
       "0        0              0                 32  \n",
       "1        0              0                 47  \n",
       "2        0              0                 72  \n",
       "3        0              0                 12  \n",
       "4        0              0                 16  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows with null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the prefix 'N' for all tail numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TailNum\"] = df[\"TailNum\"].str.slice(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to create a column in the dataframe called 'ScheduledDepTime', which has the scheduled departure time of each flight in the dataset.\n",
    "\n",
    "This computed by subtracting the departure delay (in minutes) from the actual departure time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scheduled_dep_time_col():\n",
    "    # Convert 'DepTime' to string type\n",
    "    df[\"DepTime\"] = df[\"DepTime\"].astype(str)\n",
    "\n",
    "    # Pad 'DepTime' with leading zeros to ensure it has 4 digits\n",
    "    df[\"DepTime\"] = df[\"DepTime\"].str.zfill(4)\n",
    "\n",
    "    # Replace '2400' with '0000' in 'DepTime'\n",
    "    df[\"DepTime\"] = df[\"DepTime\"].replace(\"2400\", \"0000\")\n",
    "\n",
    "    # Convert 'DepTime' column to datetime format\n",
    "    df[\"DepTime\"] = pd.to_datetime(df[\"DepTime\"], format=\"%H%M\")\n",
    "\n",
    "    # Subtract 'DepDelay' from 'DepTime'\n",
    "    df[\"ScheduledDepTime\"] = df.apply(\n",
    "        lambda row: row[\"DepTime\"] - timedelta(minutes=row[\"DepDelay\"]), axis=1\n",
    "    )\n",
    "\n",
    "    # Convert 'ScheduledDepTime' back to the original format\n",
    "    df[\"ScheduledDepTime\"] = df[\"ScheduledDepTime\"].dt.strftime(\"%H%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to recompute the scheduled elpased time of each flight.\n",
    "\n",
    "This is computed by subtracting the scheduled departure time from the scheduled arrival time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_scheduled_elapsed_time():\n",
    "    # Rename the 'CRSArrTime' column to 'ScheduledArrTime'\n",
    "    df.rename(columns={\"CRSArrTime\": \"ScheduledArrTime\"}, inplace=True)\n",
    "\n",
    "    # Convert columns to string type\n",
    "    df[\"ScheduledArrTime\"] = df[\"ScheduledArrTime\"].astype(str)\n",
    "    df[\"ScheduledDepTime\"] = df[\"ScheduledDepTime\"].astype(str)\n",
    "\n",
    "    # Pad columns with leading zeros to ensure it has 4 digits\n",
    "    df[\"ScheduledArrTime\"] = df[\"ScheduledArrTime\"].str.zfill(4)\n",
    "    df[\"ScheduledDepTime\"] = df[\"ScheduledDepTime\"].str.zfill(4)\n",
    "\n",
    "    # Replace '2400' with '0000' in columns\n",
    "    df[\"ScheduledArrTime\"] = df[\"ScheduledArrTime\"].replace(\"2400\", \"0000\")\n",
    "    df[\"ScheduledDepTime\"] = df[\"ScheduledDepTime\"].replace(\"2400\", \"0000\")\n",
    "\n",
    "    # Convert columns to datetime format\n",
    "    df[\"ScheduledArrTime\"] = pd.to_datetime(df[\"ScheduledArrTime\"], format=\"%H%M\")\n",
    "    df[\"ScheduledDepTime\"] = pd.to_datetime(df[\"ScheduledDepTime\"], format=\"%H%M\")\n",
    "\n",
    "    # Calculate the scheduled elapsed time and create a new column 'ScheduledElapsedTime'\n",
    "    df[\"ScheduledElapsedTime\"] = (\n",
    "        (\n",
    "            df[\"ScheduledArrTime\"] - df[\"ScheduledDepTime\"] + pd.Timedelta(days=1)\n",
    "        ).dt.total_seconds()\n",
    "        / 60\n",
    "    ).astype(int)\n",
    "\n",
    "    # Use modulo operation to limit the elapsed time within 24 hours\n",
    "    df[\"ScheduledElapsedTime\"] = df[\"ScheduledElapsedTime\"] % (24 * 60)\n",
    "\n",
    "    # Convert 'ScheduledArrTime' and 'ScheduledDepTime' back to the original format\n",
    "    df[\"ScheduledArrTime\"] = df[\"ScheduledArrTime\"].dt.strftime(\"%H%M\")\n",
    "    df[\"ScheduledDepTime\"] = df[\"ScheduledDepTime\"].dt.strftime(\"%H%M\")\n",
    "\n",
    "    # Convert 'ScheduledArrTime' and 'ScheduledDepTime' to int\n",
    "    df[\"ScheduledArrTime\"] = df[\"ScheduledArrTime\"].astype(int)\n",
    "    df[\"ScheduledDepTime\"] = df[\"ScheduledDepTime\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to expand the 'Date' column to a 'Day' and 'Month' columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_date_col():\n",
    "    # Convert the date column to datetime\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%d-%m-%Y\")\n",
    "\n",
    "    # Create the Day, Month and Year columns\n",
    "    df[\"Day\"] = df[\"Date\"].dt.day\n",
    "    df[\"Month\"] = df[\"Date\"].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to call the google geocode API to retrieve the lat and long of the airports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode(address):\n",
    "    url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    params = {\"address\": address, \"key\": \"AIzaSyCeWJLbBvTsN3WoA7R8y4M3DzGkKQHJp80\"}\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"results\" in data and len(data[\"results\"]) > 0:\n",
    "            location = data[\"results\"][0][\"geometry\"][\"location\"]\n",
    "            return location[\"lat\"], location[\"lng\"]\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to call an API to get weather conditions for a certain lat and long, start and end dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_conditions(lat, long, start_date, end_date):\n",
    "    # Setup the Open-Meteo API client with cache and retry on error\n",
    "    cache_session = requests_cache.CachedSession(\".cache\", expire_after=-1)\n",
    "    retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "    openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "    # Make sure all required weather variables are listed here\n",
    "    # The order of variables in hourly or daily is important to assign them correctly below\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": long,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": [\n",
    "            \"temperature_2m\",\n",
    "            \"precipitation\",\n",
    "            \"rain\",\n",
    "            \"snowfall\",\n",
    "            \"wind_speed_10m\",\n",
    "            \"wind_direction_10m\",\n",
    "        ],\n",
    "    }\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "    # Process hourly data. The order of variables needs to be the same as requested.\n",
    "    hourly = responses[0].Hourly()\n",
    "    hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()  # type:ignore\n",
    "    hourly_precipitation = hourly.Variables(1).ValuesAsNumpy()  # type:ignore\n",
    "    hourly_rain = hourly.Variables(2).ValuesAsNumpy()  # type:ignore\n",
    "    hourly_snowfall = hourly.Variables(3).ValuesAsNumpy()  # type:ignore\n",
    "    hourly_wind_speed_10m = hourly.Variables(4).ValuesAsNumpy()  # type:ignore\n",
    "    hourly_wind_direction_10m = hourly.Variables(5).ValuesAsNumpy()  # type:ignore\n",
    "\n",
    "    hourly_data = {\n",
    "        \"date\": pd.date_range(\n",
    "            start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),  # type: ignore\n",
    "            end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),  # type: ignore\n",
    "            freq=pd.Timedelta(seconds=hourly.Interval()),  # type: ignore\n",
    "            inclusive=\"left\",\n",
    "        )\n",
    "    }\n",
    "    hourly_data[\"Temperature\"] = hourly_temperature_2m  # type:ignore\n",
    "    hourly_data[\"Precipitation\"] = hourly_precipitation  # type:ignore\n",
    "    hourly_data[\"Rain\"] = hourly_rain  # type:ignore\n",
    "    hourly_data[\"SnowFall\"] = hourly_snowfall  # type:ignore\n",
    "    hourly_data[\"WindSpeed\"] = hourly_wind_speed_10m  # type:ignore\n",
    "    hourly_data[\"WindDirection\"] = hourly_wind_direction_10m  # type:ignore\n",
    "\n",
    "    return pd.DataFrame(data=hourly_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to correct airport names so that the google API could get their coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_airport_names():\n",
    "    df[\"Org_Airport\"] = df[\"Org_Airport\"].replace(\n",
    "        {\n",
    "            \"Rogue Valley International Airport\": \"Rogue Valley International Medford Airport\"\n",
    "        }\n",
    "    )\n",
    "    df[\"Org_Airport\"] = df[\"Org_Airport\"].replace(\n",
    "        {\n",
    "            \"Gen. Edward Lawrence Logan International Airport\": \"Boston Logan International Airport\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df[\"Dest_Airport\"] = df[\"Dest_Airport\"].replace(\n",
    "        {\n",
    "            \"Rogue Valley International Airport\": \"Rogue Valley International Medford Airport\"\n",
    "        }\n",
    "    )\n",
    "    df[\"Dest_Airport\"] = df[\"Dest_Airport\"].replace(\n",
    "        {\n",
    "            \"Gen. Edward Lawrence Logan International Airport\": \"Boston Logan International Airport\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to insert the weather conditions columns for departure and arrival\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_weather_conditions_cols():\n",
    "    weather_columns = [\n",
    "        \"Temperature\",\n",
    "        \"WindSpeed\",\n",
    "        \"WindDirection\",\n",
    "        \"Precipitation\",\n",
    "        \"Rain\",\n",
    "        \"SnowFall\",\n",
    "    ]\n",
    "    for prefix in [\"Dep\", \"Arr\"]:\n",
    "        for column in weather_columns:\n",
    "            df[f\"{prefix}{column}\"] = None\n",
    "    return weather_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to create a dictionary having all the unique airports as keys and empty dataframes as values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_locations_dict(unique_locations):\n",
    "    locations_dict = {value: pd.DataFrame() for value in unique_locations}\n",
    "    for i in range(len(unique_locations)):\n",
    "        print(i)\n",
    "        print(unique_locations[i])\n",
    "        lat, long = geocode(unique_locations[i])\n",
    "\n",
    "        print(\"lat: \" + str(lat))\n",
    "        print(\"long: \" + str(long))\n",
    "        print()\n",
    "        if lat != None and long != None:\n",
    "            locations_dict[unique_locations[i]] = get_weather_conditions(\n",
    "                lat, long, \"2019-01-01\", \"2019-06-30\"\n",
    "            )\n",
    "    return locations_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to add the weather conditions to the df according to the location and datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weather_condtions(locations_dict, weather_columns):\n",
    "    j = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if (\n",
    "            not locations_dict[row[\"Org_Airport\"]].empty\n",
    "            and not locations_dict[row[\"Dest_Airport\"]].empty\n",
    "        ):\n",
    "            print(j)\n",
    "            # Create temporary DataFrames for the operation\n",
    "            df_row = pd.DataFrame(row).transpose().copy()\n",
    "            dep_df = locations_dict[row[\"Org_Airport\"]].copy()\n",
    "            arr_df = locations_dict[row[\"Dest_Airport\"]].copy()\n",
    "\n",
    "            df_row[\"ScheduledDepTime\"] = df_row[\"ScheduledDepTime\"].astype(str)\n",
    "            df_row[\"ScheduledDepTime\"] = df_row[\"ScheduledDepTime\"].str.zfill(4)\n",
    "\n",
    "            df_row[\"ScheduledArrTime\"] = df_row[\"ScheduledArrTime\"].astype(str)\n",
    "            df_row[\"ScheduledArrTime\"] = df_row[\"ScheduledArrTime\"].str.zfill(4)\n",
    "\n",
    "            df_row[\"ScheduledDepTime\"] = pd.to_timedelta(\n",
    "                str(df_row[\"ScheduledDepTime\"].values[0])[:2]\n",
    "                + \":\"\n",
    "                + str(df_row[\"ScheduledDepTime\"].values[0])[2:]\n",
    "                + \":00\"\n",
    "            )\n",
    "            df_row[\"ScheduledArrTime\"] = pd.to_timedelta(\n",
    "                str(df_row[\"ScheduledArrTime\"].values[0])[:2]\n",
    "                + \":\"\n",
    "                + str(df_row[\"ScheduledArrTime\"].values[0])[2:]\n",
    "                + \":00\"\n",
    "            )\n",
    "\n",
    "            df_row[\"DepDateTime\"] = df_row[\"Date\"] + df_row[\"ScheduledDepTime\"]\n",
    "            df_row[\"ArrDateTime\"] = df_row[\"Date\"] + df_row[\"ScheduledArrTime\"]\n",
    "\n",
    "            # Convert 'date' column in df2 to datetime format without timezone\n",
    "            dep_df[\"date\"] = dep_df[\"date\"].dt.tz_convert(None)\n",
    "            arr_df[\"date\"] = arr_df[\"date\"].dt.tz_convert(None)\n",
    "\n",
    "            # # Extract date and hour from 'DateTime' in df1 and 'date' in df2\n",
    "            df_row[\"DepDateTime\"] = pd.to_datetime(df_row[\"DepDateTime\"]).dt.floor(\"h\")\n",
    "            df_row[\"ArrDateTime\"] = pd.to_datetime(df_row[\"ArrDateTime\"]).dt.floor(\"h\")\n",
    "            dep_df[\"date\"] = dep_df[\"date\"].dt.floor(\"h\")\n",
    "            arr_df[\"date\"] = arr_df[\"date\"].dt.floor(\"h\")\n",
    "\n",
    "            # # Merge the two DataFrames on the datetime column\n",
    "            dep_weather = pd.merge(\n",
    "                df_row, dep_df, left_on=\"DepDateTime\", right_on=\"date\"\n",
    "            )\n",
    "            arr_weather = pd.merge(\n",
    "                df_row, arr_df, left_on=\"ArrDateTime\", right_on=\"date\"\n",
    "            )\n",
    "\n",
    "            # # Drop the temporary columns\n",
    "            dep_weather = dep_weather.drop(columns=[\"DepDateTime\", \"date\"])\n",
    "            arr_weather = arr_weather.drop(columns=[\"ArrDateTime\", \"date\"])\n",
    "\n",
    "            for column in weather_columns:\n",
    "                df.at[index, f\"Dep{column}\"] = dep_weather[column][0]\n",
    "                df.at[index, f\"Arr{column}\"] = arr_weather[column][0]\n",
    "\n",
    "            j += 1\n",
    "        else:\n",
    "            for column in weather_columns:\n",
    "                df.at[index, f\"Dep{column}\"] = 0\n",
    "                df.at[index, f\"Arr{column}\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to insert and fill the weather conditions columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_conditons_cols():\n",
    "    correct_airport_names()\n",
    "\n",
    "    unique_locations = (\n",
    "        pd.concat([df[\"Org_Airport\"], df[\"Dest_Airport\"]]).unique().tolist()\n",
    "    )\n",
    "\n",
    "    # Create new columns in the DataFrame for weather conditions\n",
    "    weather_columns = insert_weather_conditions_cols()\n",
    "\n",
    "    # Create a dictionary with unique locations as keys and empty DataFrames as values\n",
    "    print(\"######### CREATING LOCATIONS DICTIONARY #########\")\n",
    "    locations_dict = create_locations_dict(unique_locations)\n",
    "\n",
    "    # Match the weather conditions with the departure and arrival times\n",
    "    print(\"######### GETTING WEATHER CONDITIONS #########\")\n",
    "    add_weather_condtions(locations_dict, weather_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to get the normalized data based on different methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_data(data, dist):\n",
    "    if dist == \"uniform\":\n",
    "        return MinMaxScaler().fit_transform(data)\n",
    "    elif dist == \"norm\":\n",
    "        return StandardScaler().fit_transform(data)\n",
    "    elif dist == \"lognorm\":\n",
    "        return np.log(np.abs(data.flatten()) + 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to normalize the categorical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_distribution():\n",
    "    numeric_columns = [\n",
    "        column\n",
    "        for column in df.columns\n",
    "        if df[column].dtype != \"object\"\n",
    "        and column != \"Month\"\n",
    "        and column != \"DayOfWeek\"\n",
    "        and column != \"DepDelay\"\n",
    "    ]\n",
    "\n",
    "    columns_distributions_dict = {column: \"\" for column in numeric_columns}\n",
    "\n",
    "    for column in numeric_columns:\n",
    "        print(\"###### \" + column + \" ######\")\n",
    "\n",
    "        data = df[column].values\n",
    "\n",
    "        f = Fitter(\n",
    "            data,\n",
    "            distributions=[\n",
    "                \"lognorm\",\n",
    "                \"norm\",\n",
    "                \"uniform\",\n",
    "            ],\n",
    "        )\n",
    "        f.fit()\n",
    "        f.summary(plot=False)\n",
    "        dist = f.get_best(method=\"sumsquare_error\")\n",
    "        best_dist = \"\"\n",
    "        for key in dist.keys():\n",
    "            best_dist = key\n",
    "\n",
    "        columns_distributions_dict[column] = str(best_dist)\n",
    "        print(column)\n",
    "        print(f\"Best Distribution: {best_dist}\")\n",
    "        print()\n",
    "\n",
    "    return columns_distributions_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to normalize the categorical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize():\n",
    "    columns_distributions_dict = get_best_distribution()\n",
    "\n",
    "    for column in columns_distributions_dict.keys():\n",
    "        data = np.array(df[column]).reshape(-1, 1)\n",
    "        df[column] = get_normalized_data(\n",
    "            data=data, dist=columns_distributions_dict[column]\n",
    "        )\n",
    "    return columns_distributions_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to get the label and the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_and_features():\n",
    "    LABEL_NAME = \"DepDelay\"\n",
    "    LABEL_COL = df[LABEL_NAME]\n",
    "\n",
    "    FEATURES_NAMES = [col for col in df.columns if col != LABEL_NAME]\n",
    "    FEATURES_COLS = df[FEATURES_NAMES]\n",
    "\n",
    "    return LABEL_NAME, LABEL_COL, FEATURES_NAMES, FEATURES_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to one hot encode the categorical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHotEncode():\n",
    "    OHE = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False).set_output(\n",
    "        transform=\"pandas\"\n",
    "    )\n",
    "    columns_to_OHE = [\n",
    "        \"TailNum\",\n",
    "        \"UniqueCarrier\",\n",
    "        \"Origin\",\n",
    "        \"Dest\",\n",
    "        \"DayOfWeek\",\n",
    "        \"Month\",\n",
    "    ]\n",
    "    Transformed = OHE.fit_transform(df[columns_to_OHE])  # type:ignore\n",
    "\n",
    "    return pd.concat([df, Transformed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_scheduled_dep_time_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_scheduled_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_date_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_weather_conditons_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAME, LABEL_COL, FEATURES_NAMES, FEATURES_COLS = get_label_and_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = OneHotEncode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the unecessary columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\n",
    "    columns=[\n",
    "        \"ArrTime\",\n",
    "        \"Airline\",\n",
    "        \"FlightNum\",\n",
    "        \"ActualElapsedTime\",\n",
    "        \"AirTime\",\n",
    "        \"ArrDelay\",\n",
    "        \"Org_Airport\",\n",
    "        \"Dest_Airport\",\n",
    "        \"TaxiIn\",\n",
    "        \"TaxiOut\",\n",
    "        \"Cancelled\",\n",
    "        \"CancellationCode\",\n",
    "        \"Diverted\",\n",
    "        \"CarrierDelay\",\n",
    "        \"WeatherDelay\",\n",
    "        \"NASDelay\",\n",
    "        \"LateAircraftDelay\",\n",
    "        \"SecurityDelay\",\n",
    "        \"Date\",\n",
    "        \"DepTime\",\n",
    "        \"CRSElapsedTime\",\n",
    "        \"Month\",\n",
    "        \"DayOfWeek\",\n",
    "        \"UniqueCarrier\",\n",
    "        \"TailNum\",\n",
    "        \"Origin\",\n",
    "        \"Dest\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 481895 entries, 0 to 481894\n",
      "Columns: 4073 entries, ScheduledArrTime to Month_6\n",
      "dtypes: float64(12), int64(6), int8(4055)\n",
      "memory usage: 1.9 GB\n"
     ]
    }
   ],
   "source": [
    "columns_to_convert = df.columns[\n",
    "    17:-1\n",
    "]  # Columns from 17th column up to but excluding the last column\n",
    "df[columns_to_convert] = df[columns_to_convert].astype(\"int8\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cleaned df into another csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../Cleaned_Dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the df info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the head of the df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Satistics and Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x=LABEL_NAME, bins=75)\n",
    "for column in FEATURES_NAMES:\n",
    "    sns.displot(data=FEATURES_COLS, x=column, bins=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make scatter plots for each column against the 'DepDelay' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the width of the figure\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(FEATURES_NAMES), ncols=1, figsize=(5, len(FEATURES_NAMES) * 5)\n",
    ")\n",
    "\n",
    "for i in range(0, len(FEATURES_NAMES)):\n",
    "    axes[i].scatter(FEATURES_COLS[FEATURES_NAMES[i]], LABEL_COL)\n",
    "    axes[i].set_title(f\"{LABEL_NAME} vs {FEATURES_NAMES[i]}\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the correlations between the 'DepDelay' column and all the other columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numeric_features = FEATURES_COLS.select_dtypes(exclude=[\"object\"])\n",
    "\n",
    "# Calculate the correlation of the label column with the other numeric columns\n",
    "correlation = numeric_features.corrwith(LABEL_COL).to_frame(\"Correlation with DepDelay\")\n",
    "correlation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
