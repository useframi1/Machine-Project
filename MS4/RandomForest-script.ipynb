{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from binning import bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the train and test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"Training_set.csv\")\n",
    "test_df = pd.read_csv(\"Testing_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert training set OHE columns to int8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 385516 entries, 0 to 385515\n",
      "Columns: 4073 entries, ScheduledArrTime to DepDelay\n",
      "dtypes: float64(12), int64(6), int8(4055)\n",
      "memory usage: 1.5 GB\n"
     ]
    }
   ],
   "source": [
    "columns_to_convert = train_df.columns[\n",
    "    17:-1\n",
    "]  # Columns from 17th column up to but excluding the last column\n",
    "train_df[columns_to_convert] = train_df[columns_to_convert].astype(\"int8\")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert testing set OHE columns to int8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 96379 entries, 0 to 96378\n",
      "Columns: 4073 entries, ScheduledArrTime to DepDelay\n",
      "dtypes: float64(12), int64(6), int8(4055)\n",
      "memory usage: 385.9 MB\n"
     ]
    }
   ],
   "source": [
    "columns_to_convert = test_df.columns[\n",
    "    17:-1\n",
    "]  # Columns from 17th column up to but excluding the last column\n",
    "test_df[columns_to_convert] = test_df[columns_to_convert].astype(\"int8\")\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training features and label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.iloc[:, :-1].to_numpy()\n",
    "y_train = train_df.iloc[:, -1].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the testing features and label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.iloc[:, :-1].to_numpy()\n",
    "y_test = test_df.iloc[:, -1].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a 5-fold cross-validation object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bin the label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binned = bin(y_train)\n",
    "y_test_binned = bin(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new RandomForest classifier with the best parameter values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_jobs=5, class_weight={0: 10, 1: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize scores lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions = []\n",
    "recalls = []\n",
    "accuracies = []\n",
    "f1_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using 5-fold cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### FOLD: 1 #####\n",
      "Precision = 0.6344091621191714\n",
      "Recall = 0.6524434529985474\n",
      "Accuracy = 0.6524434529985474\n",
      "F1 score = 0.6318950230324862\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.36      0.43     28516\n",
      "           1       0.69      0.83      0.75     48588\n",
      "\n",
      "    accuracy                           0.65     77104\n",
      "   macro avg       0.62      0.59      0.59     77104\n",
      "weighted avg       0.63      0.65      0.63     77104\n",
      "\n",
      "##### FOLD: 2 #####\n",
      "Precision = 0.6328966030406921\n",
      "Recall = 0.6510252519357224\n",
      "Accuracy = 0.6510252519357224\n",
      "F1 score = 0.6314776839781701\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.36      0.43     28427\n",
      "           1       0.69      0.82      0.75     48676\n",
      "\n",
      "    accuracy                           0.65     77103\n",
      "   macro avg       0.61      0.59      0.59     77103\n",
      "weighted avg       0.63      0.65      0.63     77103\n",
      "\n",
      "##### FOLD: 3 #####\n",
      "Precision = 0.6321077059365956\n",
      "Recall = 0.6505842833612181\n",
      "Accuracy = 0.6505842833612181\n",
      "F1 score = 0.6296758101358585\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.35      0.43     28513\n",
      "           1       0.68      0.83      0.75     48590\n",
      "\n",
      "    accuracy                           0.65     77103\n",
      "   macro avg       0.61      0.59      0.59     77103\n",
      "weighted avg       0.63      0.65      0.63     77103\n",
      "\n",
      "##### FOLD: 4 #####\n",
      "Precision = 0.6333998301889291\n",
      "Recall = 0.652075794716159\n",
      "Accuracy = 0.652075794716159\n",
      "F1 score = 0.6317251721522461\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.35      0.43     28303\n",
      "           1       0.69      0.82      0.75     48800\n",
      "\n",
      "    accuracy                           0.65     77103\n",
      "   macro avg       0.61      0.59      0.59     77103\n",
      "weighted avg       0.63      0.65      0.63     77103\n",
      "\n",
      "##### FOLD: 5 #####\n",
      "Precision = 0.6336669408838478\n",
      "Recall = 0.6520239160603348\n",
      "Accuracy = 0.6520239160603348\n",
      "F1 score = 0.6310879499474126\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.35      0.43     28480\n",
      "           1       0.69      0.83      0.75     48623\n",
      "\n",
      "    accuracy                           0.65     77103\n",
      "   macro avg       0.62      0.59      0.59     77103\n",
      "weighted avg       0.63      0.65      0.63     77103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold = 1\n",
    "for train, test in kf.split(X_train, y_train_binned):\n",
    "    print(f\"##### FOLD: {fold} #####\")\n",
    "\n",
    "    # Fit the model\n",
    "    rfc.fit(X_train[train], y_train_binned[train])\n",
    "\n",
    "    # Predict on the test set\n",
    "    predictions = rfc.predict(X_train[test])\n",
    "\n",
    "    # Evaluate the model\n",
    "    precision = precision_score(\n",
    "        y_true=y_train_binned[test],\n",
    "        y_pred=predictions,\n",
    "        zero_division=0,\n",
    "        average=\"weighted\",\n",
    "    )\n",
    "    recall = recall_score(\n",
    "        y_true=y_train_binned[test],\n",
    "        y_pred=predictions,\n",
    "        zero_division=0,\n",
    "        average=\"weighted\",\n",
    "    )\n",
    "    accuracy = accuracy_score(y_true=y_train_binned[test], y_pred=predictions)\n",
    "    f1 = f1_score(\n",
    "        y_true=y_train_binned[test],\n",
    "        y_pred=predictions,\n",
    "        zero_division=0,\n",
    "        average=\"weighted\",\n",
    "    )\n",
    "\n",
    "    # Store the result\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    accuracies.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Print the scores for each fold\n",
    "    print(f\"Precision = {precision}\")\n",
    "    print(f\"Recall = {recall}\")\n",
    "    print(f\"Accuracy = {accuracy}\")\n",
    "    print(f\"F1 score = {f1}\\n\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_true=y_train_binned[test], y_pred=predictions, zero_division=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the mean scores of the folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores:\n",
      "Mean Precision = 0.6332960484338471\n",
      "Mean Recall = 0.6516305398143963\n",
      "Mean Accuracy = 0.6516305398143963\n",
      "Mean F1 score = 0.6311723278492347\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Scores:\")\n",
    "print(f\"Mean Precision = {np.mean(precisions)}\")\n",
    "print(f\"Mean Recall = {np.mean(recalls)}\")\n",
    "print(f\"Mean Accuracy = {np.mean(accuracies)}\")\n",
    "print(f\"Mean F1 score = {np.mean(f1_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.36      0.43     35520\n",
      "           1       0.69      0.82      0.75     60859\n",
      "\n",
      "    accuracy                           0.65     96379\n",
      "   macro avg       0.61      0.59      0.59     96379\n",
      "weighted avg       0.63      0.65      0.63     96379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_test_binned, y_pred=predictions, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(\n",
    "    y_true=y_test_binned,\n",
    "    y_pred=predictions,\n",
    "    zero_division=0,\n",
    "    average=\"weighted\",\n",
    ")\n",
    "recall = recall_score(\n",
    "    y_true=y_test_binned,\n",
    "    y_pred=predictions,\n",
    "    zero_division=0,\n",
    "    average=\"weighted\",\n",
    ")\n",
    "accuracy = accuracy_score(y_true=y_test_binned, y_pred=predictions)\n",
    "f1 = f1_score(\n",
    "    y_true=y_test_binned,\n",
    "    y_pred=predictions,\n",
    "    zero_division=0,\n",
    "    average=\"weighted\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores:\n",
      "Mean Precision = 0.6338206931450611\n",
      "Mean Recall = 0.6520922607621993\n",
      "Mean Accuracy = 0.6520922607621993\n",
      "Mean F1 score = 0.6318835579414409\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Scores:\")\n",
    "print(f\"Mean Precision = {np.mean(precision)}\")\n",
    "print(f\"Mean Recall = {np.mean(recall)}\")\n",
    "print(f\"Mean Accuracy = {np.mean(accuracy)}\")\n",
    "print(f\"Mean F1 score = {np.mean(f1)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
